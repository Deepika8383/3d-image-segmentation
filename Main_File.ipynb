{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "C61wX4Cl3VOj",
        "outputId": "f906f8cc-bc3b-48ed-8dff-ff989647c4af"
      },
      "outputs": [],
      "source": [
        "# import nibabel as nib\n",
        "# from google.colab import files\n",
        "\n",
        "# # Upload a .nii.gz file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Extract the file name from the uploaded dictionary\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# # Load the NIfTI file using nibabel\n",
        "# nii_img = nib.load(file_name)\n",
        "\n",
        "# # Get the image data as a NumPy array\n",
        "# img_data = nii_img.get_fdata()\n",
        "\n",
        "# # Print the shape (Z, Y, X)\n",
        "# print(\"Volume Shape:\", img_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFyKYjTy7j28"
      },
      "outputs": [],
      "source": [
        "# import nibabel as nib\n",
        "# from google.colab import files\n",
        "\n",
        "# # Upload a .nii.gz file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Extract the file name from the uploaded dictionary\n",
        "# file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# # Load the NIfTI file using nibabel\n",
        "# nii_img = nib.load(file_name)\n",
        "\n",
        "# # Get the image data as a NumPy array\n",
        "# img_data = nii_img.get_fdata()\n",
        "\n",
        "# # Print the shape (Z, Y, X)\n",
        "# print(\"Volume Shape:\", img_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsdLcaE0F9Hc",
        "outputId": "80e90fc1-82a4-483a-bc4a-83ffd14df612"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# device_name = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "# if not device_name:\n",
        "#     raise SystemError('GPU device not found')\n",
        "\n",
        "# print(f'Found GPU at: {device_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ST9QspLek9s",
        "outputId": "42455feb-ac5b-488c-fbe3-23a1e12f17d1"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Uninstall old or conflicting versions\n",
        "# %pip uninstall numpy tensorflow keras scikit-image patchify matplotlib scikit-learn nibabel segmentation-models-3D classification-models-3D efficientnet-3D -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install fixed versions only\n",
        "# %pip install numpy==1.24.4 \n",
        "# %pip install tensorflow==2.13.0 \n",
        "# %pip install keras==2.13.1 \n",
        "# %pip install scikit-image==0.21.0\n",
        "# %pip install patchify==0.2.3 \n",
        "# %pip install matplotlib==3.7.1 \n",
        "# %pip install scikit-learn==1.3.0 \n",
        "# %pip install nibabel==5.1.0 \n",
        "# %pip install segmentation-models-3D==1.0.4 \n",
        "# %pip install classification-models-3D==1.0.2 \n",
        "# %pip install efficientnet-3D==1.0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGwM8CceFXsg",
        "outputId": "a4631a95-a5c5-487f-8146-86d4ae199772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segmentation Models: using `tf.keras` framework.\n",
            "numpy: 1.24.3\n",
            "tensorflow: 2.13.0\n",
            "keras: 2.13.1\n",
            "scikit-image: 0.21.0\n",
            "matplotlib: 3.7.1\n",
            "scikit-learn: 1.3.0\n",
            "nibabel: 5.1.0\n",
            "segmentation-models-3D: 1.0.4\n",
            "classification-models-3D: 1.0.0\n",
            "efficientnet-3D: 1.0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import tensorflow\n",
        "import keras\n",
        "import skimage\n",
        "import patchify\n",
        "import matplotlib\n",
        "import sklearn\n",
        "import nibabel\n",
        "import segmentation_models_3D\n",
        "import classification_models_3D\n",
        "import efficientnet_3D\n",
        "\n",
        "print(\"numpy:\", numpy.__version__)\n",
        "print(\"tensorflow:\", tensorflow.__version__)\n",
        "print(\"keras:\", keras.__version__)\n",
        "print(\"scikit-image:\", skimage.__version__)\n",
        "print(\"matplotlib:\", matplotlib.__version__)\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"nibabel:\", nibabel.__version__)\n",
        "print(\"segmentation-models-3D:\", segmentation_models_3D.__version__)\n",
        "print(\"classification-models-3D:\", classification_models_3D.__version__)\n",
        "print(\"efficientnet-3D:\", efficientnet_3D.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ere8_kg8zWhb"
      },
      "outputs": [],
      "source": [
        "# %pip install classification-models-3D\n",
        "# %pip install efficientnet-3D\n",
        "# %pip install segmentation-models-3D\n",
        "# %pip install patchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbKI7SL1GjFP",
        "outputId": "6583f2c6-6fde-457f-f7fd-21338fbed201"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_3D as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onBIxtDhGwae"
      },
      "outputs": [],
      "source": [
        "from skimage import io\n",
        "from patchify import patchify, unpatchify\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nibabel as nib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs8svBUmHQDv",
        "outputId": "36b418f3-38e0-4988-e30c-68c5cfb87d9d"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from patchify import patchify\n",
        "import math\n",
        "\n",
        "# Load the .nii.gz image and mask\n",
        "image_nifti = nib.load('./Data_Subset/images/ct_1003_image.nii.gz')\n",
        "mask_nifti = nib.load('./Data_Subset/labels/ct_1003_label.nii.gz')\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "image = np.array(image_nifti.get_fdata(), dtype=np.float32)\n",
        "mask = np.array(mask_nifti.get_fdata(), dtype=np.uint8)  # Assuming binary mask\n",
        "\n",
        "# Get original dimensions\n",
        "H, W, D = image.shape  # Shape = (Height, Width, Depth)\n",
        "print(f\"Original Image Shape: {image.shape}\")\n",
        "\n",
        "# Compute new depth (nearest multiple of 64)\n",
        "new_depth = math.ceil(D / 64) * 64\n",
        "depth_padding = new_depth - D\n",
        "\n",
        "# Pad depth dimension with zeros (only in the last axis)\n",
        "image_padded = np.pad(image, ((0, 0), (0, 0), (0, depth_padding)), mode='constant', constant_values=0)\n",
        "mask_padded = np.pad(mask, ((0, 0), (0, 0), (0, depth_padding)), mode='constant', constant_values=0)\n",
        "\n",
        "print(f\"Padded Image Shape: {image_padded.shape}\")\n",
        "\n",
        "# Extract 3D patches of (64, 64, 64) with step size 64\n",
        "image_patches = patchify(image_padded, (64, 64, 64), step=64)\n",
        "mask_patches = patchify(mask_padded, (64, 64, 64), step=64)\n",
        "\n",
        "print(f\"Image patches shape before reshaping: {image_patches.shape}\")\n",
        "print(f\"Mask patches shape before reshaping: {mask_patches.shape}\")\n",
        "\n",
        "# Reshape patches for training\n",
        "image_patches = image_patches.reshape(-1, 64, 64, 64)  # Convert to (N, 64, 64, 64)\n",
        "mask_patches = mask_patches.reshape(-1, 64, 64, 64)\n",
        "\n",
        "# Add channel dimension (1 for grayscale images)\n",
        "image_patches = image_patches[:, np.newaxis, :, :, :]  # Shape: (N, 1, 64, 64, 64)\n",
        "mask_patches = mask_patches[:, np.newaxis, :, :, :]\n",
        "\n",
        "print(f\"Final Image Patches Shape: {image_patches.shape}\")  # (N, 1, 64, 64, 64)\n",
        "print(f\"Final Mask Patches Shape: {mask_patches.shape}\")  # (N, 1, 64, 64, 64)\n",
        "\n",
        "# Total patches count\n",
        "print(f\"Total Image Patches: {image_patches.shape[0]}\")\n",
        "print(f\"Total Mask Patches: {mask_patches.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "e3bpYdm-IB_x",
        "outputId": "6ea3e97e-145a-4c85-d5bb-c3b85c3b5c1b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Select a random patch index\n",
        "patch_index = 114\n",
        "sample_patch = image_patches[patch_index, 0]  # Shape: (64, 64, 64)\n",
        "sample_mask = mask_patches[patch_index, 0]    # Corresponding mask patch\n",
        "\n",
        "# Randomly select 3 depth slices\n",
        "depth_slices = np.random.choice(range(64), 3, replace=False)\n",
        "\n",
        "# Create a 2-row grid (3 columns): Top row (image slices), Bottom row (mask slices)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "\n",
        "for i in range(3):\n",
        "    depth = depth_slices[i]\n",
        "\n",
        "    # Image slice (Top row)\n",
        "    axes[0, i].imshow(sample_patch[:, :, depth], cmap='plasma')\n",
        "    axes[0, i].set_title(f\"Image - Depth {depth}\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "\n",
        "    # Mask slice (Bottom row)\n",
        "    axes[1, i].imshow(sample_mask[:, :, depth], cmap='gray')\n",
        "    axes[1, i].set_title(f\"Mask - Depth {depth}\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "\n",
        "plt.suptitle(f\"Patch {patch_index}: Image & Mask Slices\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd3QtP_SIJdU",
        "outputId": "6aaeb56f-9062-4530-bfad-85cf32cdc592"
      },
      "outputs": [],
      "source": [
        "print(image_patches.shape)\n",
        "# [patches, channels, height , width, depth]\n",
        "# 256 patches and 1 grayscale channel , 64 x 64 x 64 - as patch volumen size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLEiDONmJf2v"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import nibabel as nib\n",
        "\n",
        "# # Load the label NIfTI file\n",
        "# mask_nifti = nib.load('/content/drive/MyDrive/Image_CHD/Data_Subset/labels/ct_1001_label.nii.gz')\n",
        "# label_data = mask_nifti.get_fdata()\n",
        "\n",
        "# # Identify unique class labels\n",
        "# unique_classes = np.unique(label_data)\n",
        "# print(f\"Segmentation classes: {unique_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_uRriksHXgW"
      },
      "outputs": [],
      "source": [
        "n_classes = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQn5vGdEHZsF"
      },
      "outputs": [],
      "source": [
        "# Convert grayscale images (1 channel) to 3 channels\n",
        "train_img = np.repeat(image_patches, 3, axis=1)  # Shape: (256, 3, 64, 64, 64)\n",
        "\n",
        "# One-hot encode masks\n",
        "train_mask_cat = to_categorical(mask_patches, num_classes=n_classes)  # Shape: (256, 1, 64, 64, 64, 8)\n",
        "\n",
        "# Remove redundant channel dimension for masks\n",
        "train_mask_cat = train_mask_cat.squeeze(1)  # New shape: (256, 64, 64, 64, 8)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    train_img, train_mask_cat, test_size=0.10, random_state=0\n",
        ")\n",
        "\n",
        "X_train = np.transpose(X_train, (0, 2, 3, 4, 1))  # (230, 64, 64, 64, 3)\n",
        "X_test = np.transpose(X_test, (0, 2, 3, 4, 1))    # (26, 64, 64, 64, 3)\n",
        "\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)  # ✅ (230, 64, 64, 64, 3)\n",
        "print(\"y_train shape:\", y_train.shape)  # ✅ (230, 64, 64, 64, 8)\n",
        "print(\"X_test shape:\", X_test.shape)    # ✅ (26, 64, 64, 64, 3)\n",
        "print(\"y_test shape:\", y_test.shape)    # ✅ (26, 64, 64, 64, 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zKm-OFaHgnQ"
      },
      "outputs": [],
      "source": [
        "#Define parameters for our model.\n",
        "\n",
        "encoder_weights = 'imagenet'\n",
        "BACKBONE = 'vgg16'  \n",
        "activation = 'softmax'\n",
        "patch_size = 64\n",
        "n_classes = 8\n",
        "channels = 3\n",
        "\n",
        "LR = 0.0001\n",
        "optim = keras.optimizers.Adam(learning_rate = LR)\n",
        "\n",
        "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
        "# Losses\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=np.array([1.0]*n_classes))\n",
        "focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + focal_loss\n",
        "\n",
        "# Metrics\n",
        "metrics = [\n",
        "    sm.metrics.IOUScore(threshold=0.5),\n",
        "    sm.metrics.FScore(threshold=0.5)\n",
        "]\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg8Ajag3Hi4f"
      },
      "outputs": [],
      "source": [
        "preprocess_input = sm.get_preprocessing(BACKBONE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmY1hgsIHi0_"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Preprocess input data - otherwise you end up with garbage resutls\n",
        "# and potentially model that does not converge.\n",
        "X_train_prep = preprocess_input(X_train)\n",
        "X_test_prep = preprocess_input(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwaE72I1HiyS"
      },
      "outputs": [],
      "source": [
        "#Define the model. Here we use Unet but we can also use other model architectures from the library.\n",
        "model = sm.Unet(BACKBONE, classes = n_classes,\n",
        "                input_shape = (patch_size, patch_size, patch_size, channels),\n",
        "                encoder_weights = encoder_weights,\n",
        "                activation = activation)\n",
        "\n",
        "model.compile(optimizer = optim, loss=total_loss, metrics=metrics)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_test_prep.shape)\n",
        "print(X_train_prep.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5kWStw8HpTx"
      },
      "outputs": [],
      "source": [
        "#Fit the model\n",
        "history=model.fit(X_train_prep,\n",
        "          y_train,\n",
        "          batch_size = 8,\n",
        "          epochs = 2,\n",
        "          verbose = 1,\n",
        "          validation_data = (X_test_prep, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3NJZ1SQHsvQ"
      },
      "outputs": [],
      "source": [
        "#Save model for future use\n",
        "model.save('./3D_model_vgg16_2epochs.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVpAkUgvHuMX"
      },
      "outputs": [],
      "source": [
        "###\n",
        "#plot the training and validation IoU and loss at each epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "acc = history.history['iou_score']\n",
        "val_acc = history.history['val_iou_score']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training IOU')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation IOU')\n",
        "plt.title('Training and validation IOU')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IOU')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For loading the model and continue training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from keras.models import load_model\n",
        "# import segmentation_models_3D as sm\n",
        "# from keras.optimizers import Adam\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Load the model\n",
        "# model = load_model('./3D_model_vgg16_2epochs.h5', compile=False)\n",
        "\n",
        "# # Step 2: Re-define loss and metrics\n",
        "# n_classes = 8\n",
        "# dice_loss = sm.losses.DiceLoss(class_weights=np.array([1/n_classes] * n_classes))  # or your custom weights\n",
        "# focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "# total_loss = dice_loss + focal_loss\n",
        "\n",
        "# metrics = [\n",
        "#     sm.metrics.IOUScore(threshold=0.5),\n",
        "#     sm.metrics.FScore(threshold=0.5)\n",
        "# ]\n",
        "\n",
        "# # Step 3: Recompile the model\n",
        "# model.compile(\n",
        "#     optimizer=Adam(learning_rate=1e-4),\n",
        "#     loss=total_loss,\n",
        "#     metrics=metrics\n",
        "# )\n",
        "\n",
        "# # Step 4: Continue training\n",
        "# history = model.fit(\n",
        "#     X_train,\n",
        "#     y_train,\n",
        "#     batch_size=8,\n",
        "#     epochs=10,  # or more\n",
        "#     validation_data=(X_test, y_test),\n",
        "#     verbose=1\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5POsM0SHwhG"
      },
      "outputs": [],
      "source": [
        "#Load the pretrained model for testing and predictions.\n",
        "from keras.models import load_model\n",
        "my_model = load_model('./3D_model_vgg16_2epochs.h5', compile=False)\n",
        "#If you load a different model do not forget to preprocess accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0mnJDfVHwdp"
      },
      "outputs": [],
      "source": [
        "# Predict on the test data\n",
        "y_pred = my_model.predict(X_test)  # Shape: (batch_size, 64, 64, 64, 8)\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_argmax = np.argmax(y_pred, axis=-1)  # Shape: (batch_size, 64, 64, 64)\n",
        "\n",
        "# Convert one-hot encoded y_test to class labels\n",
        "y_test_argmax = np.argmax(y_test, axis=-1)  # Shape: (batch_size, 64, 64, 64)\n",
        "\n",
        "# Optional: Confirm shapes\n",
        "print(\"Predicted mask shape:\", y_pred.shape)\n",
        "print(\"y_pred_argmax shape:\", y_pred_argmax.shape)\n",
        "print(\"y_test_argmax shape:\", y_test_argmax.shape)\n",
        "print(\"Unique classes \" , np.unique(y_pred_argmax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTaVcOLZHzxU"
      },
      "outputs": [],
      "source": [
        "from keras.metrics import MeanIoU\n",
        "\n",
        "# Define number of classes\n",
        "n_classes = 8 \n",
        "\n",
        "# Convert to int32 for compatibility\n",
        "y_test_argmax = y_test_argmax.astype(\"int32\")\n",
        "y_pred_argmax = y_pred_argmax.astype(\"int32\")\n",
        "\n",
        "# Initialize MeanIoU metric\n",
        "IOU_keras = MeanIoU(num_classes=n_classes)\n",
        "\n",
        "# Compute IoU\n",
        "IOU_keras.update_state(y_test_argmax.flatten(), y_pred_argmax.flatten())\n",
        "\n",
        "# Print the result\n",
        "print(\"Mean IoU =\", IOU_keras.result().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDc-8FnJHztw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "test_img_number = random.randint(0, len(X_test) - 1)  \n",
        "\n",
        "# Select a random test image\n",
        "test_img = X_test[test_img_number]\n",
        "ground_truth = y_test[test_img_number]\n",
        "\n",
        "# Expand dimensions to match model input shape (batch size = 1)\n",
        "test_img_input = np.expand_dims(test_img, axis=0)  # Shape: (1, 64, 64, 64, 3)\n",
        "\n",
        "# Apply VGG16-specific preprocessing\n",
        "test_img_input1 = preprocess_input(test_img_input)  \n",
        "\n",
        "# Make prediction\n",
        "test_pred1 = my_model.predict(test_img_input1)\n",
        "\n",
        "# Convert softmax probabilities to class labels\n",
        "test_prediction1 = np.argmax(test_pred1, axis=-1)[0]  # Remove batch dim\n",
        "\n",
        "print(\"Predicted mask shape:\", test_prediction1.shape)  # Expected: (64, 64, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D86Fn4Z9HzrU"
      },
      "outputs": [],
      "source": [
        "# Convert one-hot encoded ground truth to class labels\n",
        "ground_truth_argmax = np.argmax(ground_truth, axis=-1)  # Last axis contains class probabilities\n",
        "\n",
        "print(\"Test image shape:\", test_img.shape)  # Expected: (64, 64, 64, 3)\n",
        "print(\"Ground truth shape:\", ground_truth.shape)  # Expected: (64, 64, 64, 8)\n",
        "print(\"Ground truth (argmax) shape:\", ground_truth_argmax.shape)  # Expected: (64, 64, 64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaE6KyLdHzo0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define slice index (should be within range)\n",
        "slice_idx = 14  # Make sure it's within 0 to 63 for a 64x64x64 volume\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Display the original test image (grayscale or first channel of RGB)\n",
        "plt.subplot(131)  # Using 1 row, 3 columns\n",
        "plt.title('Testing Image')\n",
        "plt.imshow(test_img[slice_idx, :, :, 0], cmap='gray')  # First channel for visualization\n",
        "\n",
        "# Display the ground truth segmentation\n",
        "plt.subplot(132)\n",
        "plt.title('Ground Truth')\n",
        "plt.imshow(ground_truth_argmax[slice_idx, :, :], cmap='jet')  # Use 'jet' for segmentation\n",
        "\n",
        "# Display the predicted segmentation\n",
        "plt.subplot(133)\n",
        "plt.title('Model Prediction')\n",
        "plt.imshow(test_prediction1[slice_idx, :, :], cmap='jet')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ON FULL VOLUME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uzQweUpbH7Lt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original volume shape: (512, 512, 206)\n",
            "Padded volume shape: (512, 512, 256)\n",
            "Patches shape: (8, 8, 4, 64, 64, 64)\n"
          ]
        }
      ],
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from patchify import patchify\n",
        "\n",
        "# Load the volume\n",
        "nii_img = nib.load('./Data_Subset/images/ct_1003_image.nii.gz')\n",
        "large_image = nii_img.get_fdata()  # shape: (H, W, D)\n",
        "\n",
        "print(\"Original volume shape:\", large_image.shape)\n",
        "\n",
        "# Get original depth\n",
        "depth = large_image.shape[2]\n",
        "pad_depth = (64 - depth % 64) if depth % 64 != 0 else 0\n",
        "\n",
        "# Pad along the depth axis (last axis)\n",
        "padded_image = np.pad(large_image, ((0, 0), (0, 0), (0, pad_depth)), mode='constant', constant_values=0)\n",
        "print(\"Padded volume shape:\", padded_image.shape)\n",
        "\n",
        "# Extract non-overlapping patches\n",
        "patches = patchify(padded_image, (64, 64, 64), step=64)\n",
        "print(\"Patches shape:\", patches.shape)  # (x, y, z, 64, 64, 64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vc_0RshH7IO"
      },
      "outputs": [],
      "source": [
        "# Predict segmentation mask for each 3D patch\n",
        "predicted_patches = []\n",
        "\n",
        "for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        for k in range(patches.shape[2]):\n",
        "            # Extract a single grayscale patch: shape (64, 64, 64)\n",
        "            single_patch = patches[i, j, k, :, :, :]\n",
        "\n",
        "            # Convert to 3-channel input: shape (64, 64, 64, 3)\n",
        "            single_patch_3ch = np.stack((single_patch,) * 3, axis=-1)\n",
        "\n",
        "            # Expand dimensions to make it batch-shaped: shape (1, 64, 64, 64, 3)\n",
        "            single_patch_input = np.expand_dims(single_patch_3ch, axis=0)\n",
        "\n",
        "            # Preprocess input according to VGG16\n",
        "            single_patch_input = preprocess_input(single_patch_input)\n",
        "\n",
        "            # Predict segmentation output: shape (1, 64, 64, 64, 8)\n",
        "            prediction = my_model.predict(single_patch_input)\n",
        "\n",
        "            # Convert probabilities to class labels: shape (64, 64, 64)\n",
        "            prediction_argmax = np.argmax(prediction, axis=4)[0]\n",
        "\n",
        "            # Append prediction\n",
        "            predicted_patches.append(prediction_argmax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76h3LAbGH7Fk"
      },
      "outputs": [],
      "source": [
        "# Convert and reshape prediction list\n",
        "predicted_patches = np.array(predicted_patches)\n",
        "print(\"Predicted patches shape (flat):\", predicted_patches.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MwViflwH7C3"
      },
      "outputs": [],
      "source": [
        "predicted_patches_reshaped = np.reshape(predicted_patches,\n",
        "    (patches.shape[0], patches.shape[1], patches.shape[2], 64, 64, 64)\n",
        ")\n",
        "print(\"Reshaped to patch grid:\", predicted_patches_reshaped.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US6k5GoWIBDm"
      },
      "outputs": [],
      "source": [
        "# Reconstruct segmented volume\n",
        "reconstructed_image = unpatchify(predicted_patches_reshaped, padded_image.shape)\n",
        "reconstructed_image = reconstructed_image[:, :, :depth]  # Remove padding\n",
        "print(\"Final reconstructed shape:\", reconstructed_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save basic segmented volume\n",
        "reconstructed_image = reconstructed_image.astype(np.uint8)\n",
        "imsave('/content/drive/MyDrive/Colab Notebooks/data/sandstone_3d/all_images/segmented.tif', reconstructed_image)\n",
        "print(\"Saved raw segmented image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4aqQUJmIF20"
      },
      "outputs": [],
      "source": [
        "# Create binary masks for each of the 8 classes\n",
        "num_segments = 8\n",
        "final = np.zeros((*reconstructed_image.shape, num_segments), dtype=np.int8)\n",
        "\n",
        "for i in range(num_segments):\n",
        "    final[..., i] = (reconstructed_image == i).astype(np.int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ex-FUSuIM8x"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Use APEER OMETIFF library to read and write multidimensional images\n",
        "%pip install apeer-ometiff-library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCxFe7FpIM5Y"
      },
      "outputs": [],
      "source": [
        "from apeer_ometiff_library import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to OMETIFF format (T, Z, C, X, Y)\n",
        "final = np.expand_dims(final, axis=0)  # Add time axis T=1\n",
        "final = np.swapaxes(final, 2, 4)       # Move channels to 3rd dimension\n",
        "print(\"Shape for OMETIFF (T, Z, C, X, Y):\", final.shape)\n",
        "\n",
        "# Save as OMETIFF\n",
        "io.write_ometiff(\"/content/drive/MyDrive/Colab Notebooks/data/sandstone_3d/all_images/segmented_multi_channel.ome.tiff\", final)\n",
        "print(\"Multi-channel segmented OMETIFF saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXfp99FJIM0P"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of the segmented volume is: T, Z, C, X, Y \", final.shape)\n",
        "print(final.dtype)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
